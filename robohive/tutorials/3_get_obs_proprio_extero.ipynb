{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get details from RoboHive Environments\n",
    "RoboHive supports three ways to query the environment based on the nature of information required. Following information can be queried from an active environment\n",
    "1. Get observations (configured using `obs_keys`)\n",
    "2. Get proprioception (configured via `proprio_keys`)\n",
    "3. Get exteroception (configured via `visual_keys`)\n",
    "\n",
    "Let's go through them in detail one at a time. First we load an environment, preconfigured with respective keys, and step it a few times -\n",
    "(we will go through the key configuration details in another tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import robohive\n",
    "\n",
    "# create an env and reset\n",
    "env = gym.make('door_v2d-v1')\n",
    "env.reset()\n",
    "\n",
    "# Lets step is few times\n",
    "for _ in range(5):\n",
    "    allinfo_tdt = env.step(env.action_space.sample())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. get observations\n",
    "This is the most commonly used method to get the current (state based) observation from the environment. Observations are customized using `env.obs_keys` and can contain any details that the environment has access to. RoboHive doesn't put any restrictions on the information that can be provided by the environment.\n",
    "\n",
    "RoboHive env provides an `env.get_obs()` method to query observations. It triggers the following sequence of events -\n",
    "1. Uses a robot (sim/hardware) to get sensors\n",
    "2. Reconstructs (partially) observed-sim `env.sim_obsd` using (noisy) sensor data\n",
    "3. Build the full observation dictionary `env.obs_dict` using the observed-sim\n",
    "4. Build obs vector from the obs_dict (using the `env.obs_keys`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current observation vector (internally updates env.sim_obsd & env.obs_dict)\n",
    "obs = env.get_obs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. get proprioception\n",
    "This method is used to get *only proprioceptive signals* from the robohive environments. Proprioceptive signals are customized using `env.proprio_keys` and typically consist of proprioceptive sensors that are available to the agent/robots. Proprioception can be acquired in two ways     \n",
    "1. Recover from *existing* observation dictionary\n",
    "2. Update observation and get proprioception alongside (default behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover from existing observation dictionary \n",
    "time, proprio_vec, proprio_dict = env.get_proprioception(env.obs_dict)\n",
    "\n",
    "# Update observation and get proprioception alongside\n",
    "obs = env.get_obs(update_proprioception=True)\n",
    "# you can get access proprioception dictionary now\n",
    "print(env.proprio_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. get exteroception\n",
    "This method is used to get *only exteroceptive signals* from the robohive environments. exteroceptive signals typically consist of exteroceptive sensors that are available to the agent/robots. Currently we are focusing primarily on cameras as exteroceptive inputs. It can be customized using `env.visual_keys`. Expanding to other exteroceptive modalities while possible and within scope, is not a tested functionality yet.\n",
    "\n",
    "Note that exteroceptive sensors are expensive (compute + bandwidth), therefore its requires users to make explicit calls for an update. It can be acquired in two ways -\n",
    "\n",
    "1. Make an explicit call to `env.get_exteroception()`\n",
    "2. Update observation and get proprioception alongside (False by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an explicit call to update\n",
    "extero_dict = env.get_exteroception()\n",
    "\n",
    "# Update observation and get exteroception alongside\n",
    "obs = env.get_obs(update_exteroception=True)\n",
    "print(env.visual_dict.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. get everything\n",
    "If it's of interest to get all the information at once, `env_get_obs()` can be asked to make all the updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.get_obs(update_proprioception=True, update_exteroception=True)\n",
    "print(f\"time = {env.obs_dict['time']}\")\n",
    "print(f\"obs vector = {obs}\")\n",
    "print(f\"obs_dict = {env.obs_dict.keys()}\")\n",
    "print(f\"proprio_dict = {env.proprio_dict.keys()}\")\n",
    "print(f\"visual_dict = {env.visual_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. convert depth into a point cloud\n",
    "\n",
    "If you need a point cloud observation, you can convert the depth image into the point cloud in the following way.\n",
    "\n",
    "First, use env.get_visuals to obtain the depth image in the obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "obs = env.get_visuals(visual_keys=['rgb:vil_camera:224x224:2d', 'd:vil_camera:224x224:2d'])\n",
    "color = obs['rgb:vil_camera:224x224:2d']\n",
    "depth = obs['d:vil_camera:224x224:2d'][0]\n",
    "plt.imshow(color)\n",
    "plt.show()\n",
    "plt.imshow(depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the depth image into a point cloud based on camera intrinsics and extrinsics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robohive.utils.pointcloud_utils import get_point_cloud, visualize_point_cloud_from_nparray\n",
    "pcd = get_point_cloud(env, obs['d:vil_camera:224x224:2d'][0], 'vil_camera')\n",
    "\n",
    "# # Interactive point cloud visualization with open3d (requires open3d installation)\n",
    "# visualize_point_cloud_from_nparray(pcd, obs['rgb:vil_camera:224x224:2d'], vis_coordinate=True)\n",
    "\n",
    "# Visualize the point cloud with matplotlib (if you are too lazy to install open3d)\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(pcd[:,0], pcd[:,1], pcd[:,2], c=obs['rgb:vil_camera:224x224:2d'].reshape(-1, 3)/256)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical tips\n",
    "\n",
    "### 1. Why ['time', 'time'] as observation?\n",
    "Note the observation vector - it looks too simple for such an environment of this complexity. This seems a little weird!\n",
    "It is a common practice in *RoboHive* to use ['time', 'time'] as observation for envs designed for visual diversity. This is for two reasons \n",
    "1. To avoid leaking oracle information via obs to the agents studying visual generalization \n",
    "2. Single ['time'] key leads to silent singleton expansion when inputs of higher dimensions are required. Replicating the `time` keys twice helps to catch/expose this bug.\n",
    "\n",
    "### 2. Ways to ask for obs, proprioception, exteroception?\n",
    "The flexibility of RoboHive allows multiple ways to ask for information from the env. We outline a of options below - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover all info at current timestep: obs(t), rwd(t), done(t), info(t)\n",
    "obs_t, rwd_t, done_t, info_t = env.env.forward(update_proprioception=True, update_exteroception=True)\n",
    "print(f\"time = {env.obs_dict['time']}\")\n",
    "print(f\"obs vector = {obs_t}\")\n",
    "print(f\"obs_dict = {env.obs_dict.keys()}\")\n",
    "print(f\"proprio_dict = {env.proprio_dict.keys()}\")\n",
    "print(f\"visual_dict = {env.visual_dict.keys()}\")\n",
    "\n",
    "# Recover info at the next timestep: obs(t+dt), rwd(t+dt), done(t+dt), info(t+dt)\n",
    "obs_tdt, rwd_tdt, done_tdt, info_tdt = env.env.step(env.action_space.sample(), update_proprioception=True, update_exteroception=True)\n",
    "print(f\"time = {env.obs_dict['time']}\")\n",
    "print(f\"obs vector = {obs_tdt}\")\n",
    "print(f\"obs_dict = {env.obs_dict.keys()}\")\n",
    "print(f\"proprio_dict = {env.proprio_dict.keys()}\")\n",
    "print(f\"visual_dict = {env.visual_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robohive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
